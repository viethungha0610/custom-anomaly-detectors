{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing core libraries and custom estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Importing custom estimators and evaluation tool\n",
    "from CustomEstimator import MultivariateGaussian, MultivariateTDistribution, SimpleAnomalyDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data - Credit Card Fraud dataset\n",
    "#### Context on the dataset\n",
    "\n",
    "V1 to V28 are PCA components of the data. Only features 'Time' and 'Amount' are untransformed features.\n",
    "\n",
    "For more details about the dataset, visit the [Kaggle site here](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "\n",
    ">The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n",
    "The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "It contains only numerical input variables which are the result of a PCA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal only data shape:  (284315, 31)\n",
      "Fraud only data shape:  (492, 31)\n",
      "Train data shape:  (227452, 30)\n",
      "Dev data shape:  (28677, 30)\n",
      "Test data shape:  (28678, 30)\n"
     ]
    }
   ],
   "source": [
    "# Importing the Credit Card Fraud dataset\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "y_data = data.copy()['Class'].values\n",
    "original_data = data.copy()\n",
    "\n",
    "# Clean data set\n",
    "normal_only_data = data[data['Class']==0]\n",
    "print('Normal only data shape: ', normal_only_data.shape)\n",
    "# Fraud data set\n",
    "fraud_only_data = data[data['Class']==1]\n",
    "print('Fraud only data shape: ', fraud_only_data.shape)\n",
    "\n",
    "# Shuffling the data\n",
    "normal_only_data = normal_only_data.sample(frac=1, random_state=42)\n",
    "fraud_only_data = fraud_only_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# 80/10/10 data split for normal data\n",
    "train_set, dev_set, test_set = np.split(normal_only_data, [int(0.8*len(normal_only_data)), int(0.9*len(normal_only_data))])\n",
    "train_set = train_set.drop('Class', axis=1)\n",
    "\n",
    "# 50/50 data split for fraud data\n",
    "fraud_set_1, fraud_set_2 = np.split(fraud_only_data, [int(0.5*len(fraud_only_data))])\n",
    "\n",
    "# Appending fraud data to dev and test set\n",
    "dev_set = dev_set.append(fraud_set_1)\n",
    "y_dev_set = dev_set['Class']\n",
    "dev_set = dev_set.drop('Class', axis=1)\n",
    "test_set = test_set.append(fraud_set_2)\n",
    "y_test_set = test_set['Class']\n",
    "test_set = test_set.drop('Class', axis=1)\n",
    "\n",
    "# Showing shapes\n",
    "for name, data in zip(['Train data shape: ', 'Dev data shape: ', 'Test data shape: '],[train_set, dev_set, test_set]):\n",
    "    print(name, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first few rows of the data\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing models - Novelty detection\n",
    "For simplicity, I will train and test the custom and a few scikit-learn models on a novelty detection bias.\n",
    "\n",
    "Novelty detection uses clean data (normal) only to train, and predict on contaminated data. Whereas Outlier detection uses contaminated (normal + fraud) data to train.\n",
    "\n",
    "For more information, refer to scikit-learn's [Guidance](https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate models\n",
    "labels = ['Normal', 'Fraud']\n",
    "def evaluate_model(y_true, y_preds, labels=labels):\n",
    "    cm = confusion_matrix(y_true, y_preds)\n",
    "    print('Recall score:\\n', recall_score(y_true, y_preds))\n",
    "    print('Precision score:\\n', precision_score(y_true, y_preds))\n",
    "    print('Confusion matrix:\\n')\n",
    "    cm_df = pd.DataFrame({'Normal (predicted)': (cm[0, 0], cm[1, 0]),\n",
    "                         'Fraud (predicted)': (cm[0, 1], cm[1, 1])},\n",
    "                        index=['Normal (true)', 'Fraud (true)'])\n",
    "    print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateGaussian(epsilon=9.3132257461548e-40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Multivariate Gaussian Anomaly Detector\n",
    "mvg = MultivariateGaussian(epsilon=0.05**30)\n",
    "mvg.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.3780487804878049\n",
      "Precision score:\n",
      " 0.1706422018348624\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               27979                452\n",
      "Fraud (true)                  153                 93\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev set\n",
    "mvg_y_dev_preds = mvg.predict(dev_set)\n",
    "evaluate_model(y_dev_set, mvg_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleAnomalyDetector(epsilon=9.3132257461548e-40)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Simple Anomaly Detector (SAD)\n",
    "# Side note: I'm still thinking about a new name for this Estimator ... suggestions are welcomed\n",
    "simple = SimpleAnomalyDetector(epsilon=0.05**30)\n",
    "simple.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.8414634146341463\n",
      "Precision score:\n",
      " 0.22162740899357602\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               27704                727\n",
      "Fraud (true)                   39                207\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev Set\n",
    "y_simple_dev_preds = simple.predict(dev_set)\n",
    "evaluate_model(y_dev_set, y_simple_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateTDistribution(df=3, epsilon=9.3132257461548e-40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Multivariate T Anomaly Detector\n",
    "mvt = MultivariateTDistribution(epsilon=0.05**30, df=3)\n",
    "mvt.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.8130081300813008\n",
      "Precision score:\n",
      " 0.5012531328320802\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               28232                199\n",
      "Fraud (true)                   46                200\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev Set\n",
    "mvt_y_dev_preds = mvt.predict(dev_set)\n",
    "evaluate_model(y_dev_set, mvt_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZH834BT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LocalOutlierFactor(algorithm='auto', contamination='legacy', leaf_size=30,\n",
       "                   metric='euclidean', metric_params=None, n_jobs=None,\n",
       "                   n_neighbors=20, novelty=True, p=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Local Outlier Factor Anomaly Detector\n",
    "lof = LocalOutlierFactor(novelty=True, metric='euclidean')\n",
    "lof.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.4715447154471545\n",
      "Precision score:\n",
      " 0.03488721804511278\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               25222               3209\n",
      "Fraud (true)                  130                116\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev set\n",
    "lof_y_dev_preds = lof.predict(dev_set)\n",
    "lof_y_dev_preds[lof_y_dev_preds==1] = 0\n",
    "lof_y_dev_preds[lof_y_dev_preds==-1] = 1\n",
    "evaluate_model(y_dev_set, lof_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZH834BT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:237: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IsolationForest(behaviour='new', bootstrap=False, contamination='legacy',\n",
       "                max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "                n_jobs=None, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Isolation Forest Anomaly Detector\n",
    "ifr = IsolationForest(random_state=42, behaviour=\"new\")\n",
    "ifr.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.8861788617886179\n",
      "Precision score:\n",
      " 0.06982703395259449\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               25527               2904\n",
      "Fraud (true)                   28                218\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev Set\n",
    "ifr_y_dev_preds = ifr.predict(dev_set)\n",
    "ifr_y_dev_preds[ifr_y_dev_preds==1] = 0\n",
    "ifr_y_dev_preds[ifr_y_dev_preds==-1] = 1\n",
    "evaluate_model(y_dev_set, ifr_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Kmeans Clustering\n",
    "kmeans = KMeans(n_clusters=2, algorithm='auto')\n",
    "kmeans.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.34552845528455284\n",
      "Precision score:\n",
      " 0.006477671086724585\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               15394              13037\n",
      "Fraud (true)                  161                 85\n"
     ]
    }
   ],
   "source": [
    "# Evaluating KMeans on the Dev Set\n",
    "kmeans_y_dev_preds = kmeans.predict(dev_set)\n",
    "# Since KMeans only does clustering, we can decide which cluster would be Normal and which cluster would be Fraud\n",
    "kmeans_y_dev_preds = np.where(kmeans_y_dev_preds==1, 0, 1)\n",
    "evaluate_model(y_dev_set, kmeans_y_dev_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contaminated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZH834BT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:237: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.8800813008130082\n",
      "Precision score:\n",
      " 0.015203117868052386\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)              256267              28048\n",
      "Fraud (true)                   59                433\n"
     ]
    }
   ],
   "source": [
    "# Trying out Isolation Forest on contaminated dataset\n",
    "ifr_2 = IsolationForest(random_state=42, behaviour=\"new\")\n",
    "ifr_2_y_preds = ifr_2.fit_predict(original_data.drop('Class', axis=1))\n",
    "ifr_2_y_preds[ifr_2_y_preds==1] = 0\n",
    "ifr_2_y_preds[ifr_2_y_preds==-1] = 1\n",
    "evaluate_model(y_data, ifr_2_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Neural Network Autoencoders\n",
    "\n",
    "#### Intuition\n",
    "The basic idea behind NN Autoencoders is to learn the very low-level representations of the data. After this process hopefully the 'noise' have been minimised from the data, and the result representations (outputs of NN Autoencoders) can be used as inputs to simplier classifiers such as Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Considered using XGBoost, my it would take a while on my EY laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Train data shape:\n",
      " (100390, 31)\n",
      "NN Dev data shape:\n",
      " (20102, 31)\n"
     ]
    }
   ],
   "source": [
    "# I use smaller sample to save computational time\n",
    "train_data = normal_only_data[:100000].append(fraud_only_data[:390])\n",
    "train_data = train_data.sample(frac=1, random_state=42)\n",
    "print('NN Train data shape:\\n', train_data.shape)\n",
    "dev_data = normal_only_data[100000:120000].append(fraud_only_data[390:])\n",
    "dev_data = dev_data.sample(frac=1, random_state=42)\n",
    "print('NN Dev data shape:\\n', dev_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100390, 30) (100390,)\n",
      "(20102, 30) (20102,)\n"
     ]
    }
   ],
   "source": [
    "# Separating X and y\n",
    "X_train_nn, y_train_nn = train_data.drop('Class', axis=1, inplace=False), train_data['Class'].values\n",
    "scaler = MinMaxScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train_nn)\n",
    "print(X_train_nn.shape, y_train_nn.shape)\n",
    "X_dev_nn, y_dev_nn = dev_data.drop('Class', axis=1, inplace=False), dev_data['Class'].values\n",
    "X_dev_nn = scaler.transform(X_dev_nn)\n",
    "print(X_dev_nn.shape, y_dev_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building computational graph for NN Autocencoder\n",
    "# Input layer\n",
    "input_layer = Input(shape=(X_train_nn.shape[1],))\n",
    "\n",
    "# Encoding part\n",
    "encoded_1 = Dense(200, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoded_2 = Dense(100, activation='relu')(encoded_1)\n",
    "\n",
    "# Decoding part\n",
    "decoded_1 = Dense(100, activation='tanh')(encoded_2)\n",
    "decoded_2 = Dense(200, activation='tanh')(decoded_1)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(X_train_nn.shape[1], activation='relu')(decoded_2)\n",
    "\n",
    "# Compiling model\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 0.0474 - val_loss: 0.0066\n",
      "Epoch 2/20\n",
      "314/314 [==============================] - 1s 2ms/step - loss: 0.0039 - val_loss: 6.5883e-04\n",
      "Epoch 3/20\n",
      "314/314 [==============================] - 1s 2ms/step - loss: 6.0444e-04 - val_loss: 5.1009e-04\n",
      "Epoch 4/20\n",
      "314/314 [==============================] - 1s 2ms/step - loss: 4.7754e-04 - val_loss: 4.4006e-04\n",
      "Epoch 5/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 4.1329e-04 - val_loss: 3.8284e-04\n",
      "Epoch 6/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 3.7264e-04 - val_loss: 3.3549e-04\n",
      "Epoch 7/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 3.3355e-04 - val_loss: 4.3245e-04\n",
      "Epoch 8/20\n",
      "314/314 [==============================] - 1s 2ms/step - loss: 3.0845e-04 - val_loss: 3.4789e-04\n",
      "Epoch 9/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.8654e-04 - val_loss: 2.6336e-04\n",
      "Epoch 10/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 3.1720e-04 - val_loss: 7.6879e-04\n",
      "Epoch 11/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.6817e-04 - val_loss: 2.4175e-04\n",
      "Epoch 12/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.5008e-04 - val_loss: 2.2732e-04\n",
      "Epoch 13/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.3530e-04 - val_loss: 2.1445e-04\n",
      "Epoch 14/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.2524e-04 - val_loss: 2.1075e-04\n",
      "Epoch 15/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.1778e-04 - val_loss: 2.2016e-04\n",
      "Epoch 16/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.1531e-04 - val_loss: 1.8815e-04\n",
      "Epoch 17/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.0089e-04 - val_loss: 1.8479e-04\n",
      "Epoch 18/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 2.1164e-04 - val_loss: 1.8531e-04\n",
      "Epoch 19/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 1.8048e-04 - val_loss: 1.6921e-04\n",
      "Epoch 20/20\n",
      "314/314 [==============================] - 1s 3ms/step - loss: 1.8144e-04 - val_loss: 1.7656e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x159178af688>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "autoencoder.fit(X_train_nn, X_train_nn,\n",
    "                batch_size=256, epochs=20,\n",
    "                shuffle=True, validation_split=0.2\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 200)               6200      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 30)                6030      \n",
      "=================================================================\n",
      "Total params: 62,630\n",
      "Trainable params: 62,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a computation graph to get the hidden representations of X\n",
    "hidden_representation = Sequential()\n",
    "hidden_representation.add(autoencoder.layers[0])\n",
    "hidden_representation.add(autoencoder.layers[1])\n",
    "hidden_representation.add(autoencoder.layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the hidden representations of X_train\n",
    "rep_X_train = hidden_representation.predict(X_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression - mapping NN Training output to y\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(rep_X_train, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now moving on to the Dev Set\n",
    "rep_X_dev = hidden_representation.predict(X_dev_nn)\n",
    "nn_y_dev_preds = logreg.predict(rep_X_dev)\n",
    "np.unique(nn_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.5882352941176471\n",
      "Precision score:\n",
      " 0.9230769230769231\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               19995                  5\n",
      "Fraud (true)                   42                 60\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Dev Set\n",
    "evaluate_model(y_dev_nn, nn_y_dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZH834BT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier - training on hidden representations of X\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(rep_X_train, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score:\n",
      " 0.803921568627451\n",
      "Precision score:\n",
      " 0.9425287356321839\n",
      "Confusion matrix:\n",
      "\n",
      "               Normal (predicted)  Fraud (predicted)\n",
      "Normal (true)               19995                  5\n",
      "Fraud (true)                   20                 82\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on the Dev Set\n",
    "nn_y_dev_preds_rf = rf.predict(rep_X_dev)\n",
    "evaluate_model(y_dev_nn, nn_y_dev_preds_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
